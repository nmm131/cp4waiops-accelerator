{
    "connectionmode": "live",
    "knativehistory": "outbound-kn-channel.camel-integrations-scm.svc.cluster.local",
    "specversion": "1.0",
    "id": "13BE410A37DFF0A-000000000000002C",
    "source": "https://dev82w95.service-now.com/change_request.do?sysparm_query=number=CHG0000055",
    "time": "2021-01-08T22:02:24.402Z",
    "type": "com.ibm.sdlc.snow.changerequest.discovered",
    "tooltype": "com.ibm.sdlc.type.snow",
    "datacontenttype": "application/json",
    "data": {
        "connection_id": "conn_1",
        "instance" : "dev82395.service-now.com",
        "sys_id" : "780f4934727811ebb072c8e0eb1474cb",
        "number" : "CHG0682318",
        "assigned_to" : "mariam.saood@ibm.com",
        "sys_created_by" : "don.kutach@ibm.com",
        "sys_domain" : "crn:v1:staging:public::us-south::::",
        "business_service" : "Virtual Private Cloud",
        "type" : "standard",
        "state" : "Closed",
        "short_description" : "is-ng-vpc IBM Yellow Staging 1 (YS1) STAGING Standard 2020-05-28 18:00:00 UTC",
        "impact" : "No impact expected.",
        "reason" : "Upgrade DAL staging (736 Only) to:\r\n\r\nhostos-config-release: 1.6.0-20200520T235636Z_92733e6\r\n\r\nSOC2 updates\r\n\r\n716 nd 726 previously updated under CHG0675190",
        "justification" : null,
        "description" : "++ If this ticket is closed with problems, a Jira ticket; type bug; project CLD; add label HotList; must be filed. ++\r\n\r\nContacts:\r\nGeneral - Dean Bair / Don Kutach\r\nKube: Patrick Sullivan, Alex Iannicelli\r\nHostos: John Depetrillo, Fernando Pizzano\r\n\r\n+ For reference, these are the test results after performing this upgrade and procedure in environments prior to taking it to the region.\r\n\r\nIntegration:\r\n\r\n\r\nAutomated Test Result ReporterAPP  3:12 PM\r\nTest Results for e4f56d1f-3caf-433e-bece-fce5f8920bcc Automated Run on TESTAUTOBOT_RIAS-SMOKE (http://9.41.33.184:3000/tests/TESTAUTOBOT_RIAS-SMOKE/e4f56d1f-3caf-433e-bece-fce5f8920bcc)\r\nTest Run ID\r\nTAB_RIAS_SMOKE_2020-05-21-19:49:16UTC_9333\r\nLink to Related Artifacts\r\nhttp://9.41.33.184:80/artifact/automation/testautobot/2020-05-21/20:12:50_940090\r\nResults Total:\r\nPASS: 144(100.00%)  FAIL: 0(0.00%) SKIP: 0(0.00%)  Total: 144\r\n\r\n\r\n####### Environment info ####### \r\n\r\n+ DAL staging MZR comprised of mzones 716, 726, and 736\r\n+ ETCD IKS: rias-ng-dal-mzr-preprod-etcd\r\n+ RIAS IKS Clusters: rias-ng-us-south-dal10-preprod, rias-ng-us-south-dal12-preprod, rias-ng-us-south-dal13-preprod\r\n+ RIAS REST URL:  us-south-stage01.iaasdev.cloud.ibm.com\r\n+ Deploy servers: dal10-staging (dal1-qz1-sr3-rk095-s04), dal12-staging (dal2-qz1-sr2-rk219-s02), dal13-staging (dal3-qz1-sr1-rk403-s04)\r\n+ You must go through the bastion hosts to reach these deployer servers.\r\n\r\n\r\n####### Assumptions ####### \r\n\r\n+ SSH Config is already set up for the deployers you are logging into per Ops documentation maintained at https://confluence.swg.usma.ibm.com:8445/pages/viewpage.action?pageId=20970930\r\n+ Git ssh keys are on the deploy servers.\r\n+ Access to a vault token\r\n\r\n####### Refresh SSH config files #######\r\n\r\nIf you generated your SSH config file prior to May 14th, you will need to regenerate the files since some of the entries were updated.  Specifically,  dal1x-preprod was updated to dal1x-staging.\r\n\r\nSee https://github.ibm.com/cloudlab/ops-inventory/blob/master/README.md for details.\r\n\r\n####### Execution #######\r\n\r\n+ Time est:\r\n++ 15 mins per mzone\r\n++ 30 mins for smoke\r\n+ The below steps (1-3) should be execute from each deploy server.\r\n+ Do steps 1-3 for each mzone, the mzones must be updated serially.\r\n+ If any errors are detected, the error needs to be understood BEFORE moving forward.\r\n\r\n\r\n####### mzone pre checks ####### \r\n\r\n1.a\r\n+ check the state of the nodes on the mzone (736 Only)\r\n\r\nkubectl get nodes | grep Not\r\n\r\n+ if any node is found to be in the NotReady state it needs to be addressed before proceeding via:\r\n+ https://github.ibm.com/cloudlab/internal-docs/blob/master/pages/runbooks/remove_sled_from_mzone.md\r\n\r\n\r\n+ cleanup failed jobs\r\nkubectl get pods --all-namespaces | grep 'genctl-storage\\|rias-leak-detect' | grep Error | awk '{print \"kubectl delete pod -n \" $1 \" \" $2 }'  | bash\r\n\r\n+ check stat state of the pods on the mzone\r\n\r\nkubectl get pods --all-namespaces | grep -v 'Complete\\|Running'\r\n\r\n+ if any pods are found to be in a bad state it needs to be addressed before proceeding with this ticket \r\n+ Any genctl-storage-monitor or rias-leak-detect pods in the Error state can just be kube deleted.\r\n\r\n\r\n1.b\r\nssh <yourid>@<deployment_server> (736 Only)\r\n\r\n+ run the gentool deploy_check tool from the deploy server.\r\n\r\ndeploy_checks.py <mzone> pre\r\n\r\n+ Verify there are no errors detected\r\n+ All error must be understood before proceeding\r\n+ Note:  If the mzone has been updated recently you can ignore the following warnings:\r\n++ iobricks has been restarted\r\n++ fabcon has been restarted\r\n++ Error. skydive service not running - can be ignored for power sleds \r\n\r\n\r\n####### The update ####### \r\n\r\n2.a\r\n(736 Only)\r\n+ Execute the follow commands on the deploy server:\r\n\r\nexport ARTIF_USER=somebody@us.ibm.com\r\n export ARTIF_APIKEY=your_secret_key\r\n\r\ncd && rm -rf pso-deployment && git clone git@github.ibm.com:dgbair/pso-deployment.git && cd pso-deployment\r\n\r\n./CHG0675190.sh. # reuse same script\r\n\r\n\r\n+ When the deployment is finished, you'll see a formatted message. Copy and paste it into the Work Notes field of this CR as well as the slack channel.\r\n+ Note: Be sure you run smoke sanity (as shown below) and step 3, before moving to the next mzone.  \r\n+ If something fails, it needs to be understood before moving forward to the next mzone.\r\n\r\n+ Validation with a quick sanity smoke run. ( region-dal-g5-staging-mzr slack channel )\r\n\r\n+ Leave off the comment at the end\r\n@testautobot rias-smoke save-artifacts report-es sanity mzone-name:us-south-3    (for mz736 update)\r\n\r\n\r\n####### mzone post checks ####### \r\n\r\n3.a\r\n+ On the deploy server (736 Only)\r\n\r\n+ run the gentool deploy_check tool from the deploy server.\r\n\r\n+ PLEASE NOTE,:you will need to be sure gentool is at the latest level. ( sudo salt-call state.apply gentool )\r\n+ or it will fail reading MaxSessions for the sysops user\r\n\r\ndeploy_checks.py <mzone> post\r\n\r\n+ Verify there are no errors detected\r\n+ All error must be understood before proceeding\r\n+ Note:  If the mzone has been updated recently you can ignore the following warnings:\r\n++ iobricks has been restarted\r\n++ fabcon has been restarted\r\n++ Error. skydive service not running - can be ignored for power sleds\r\n\r\n####### Final validation ####### \r\n\r\n4.a\r\n+ Did you repeat steps 1-3 for each mzone?\r\n\r\n+ Post below command to the region-dal-g5-staging-mzr slack channel:  (including the comment)\r\n\r\n@TestAutobot configure hostos-config-release: 1.6.0-20200520T235636Z_92733e6\r\n\r\n4.b\r\n+ Run smoke. ( region-dal-g5-staging-mzr slack channel)\r\n@testautobot rias-smoke save-artifacts report-es\r\n\r\n\r\n++ If this ticket is closed with problems, a Jira ticket; type bug; project CLD; add label HotList; must be filed. ++",
        "backout_plan" : "On the mzone which failed:\r\n\r\nbak.1 \r\n+ On the deploy server\r\n+ run gentool from the deploy server.\r\n\r\nsource set-mzone <mzone>\r\ndeploy_checks.py <mzone> post\r\n\r\nbak.2\r\n+ Run smoke. ( region-dal-g5-staging-mzr slack channel)\r\n+ Run one of the below commands depending on the mzone which failed to update.\r\n@testautobot rias-smoke save-artifacts report-es mzone-name:us-south-1    (for mz716 update)\r\n@testautobot rias-smoke save-artifacts report-es mzone-name:us-south-2    (for mz726 update)\r\n@testautobot rias-smoke save-artifacts report-es mzone-name:us-south-3    (for mz736 update)\r\n\r\nbak.3\r\n+ Open a Jira ticket; type bug; project CLD; add label HotList; \r\n+ Include deployment logs, the gentool output, and the smoke results.\r\n\r\n\r\nBacking out code is not in the design for the upgrade tooling for VPC Gen2 and backout is not an option, only forward movement.\r\n\r\nThe plan if there are issues with this CR ticket will be to file a Jira ticket type bug in the CLD project and label it HotList. \r\nAnd then reference that ticket in the Closing Notes field of this CR.\r\nWhen you announce the failure of the CR in channel, the team will know to look at the CR for details. \r\nFurther, the hotlist flagged items are reviewed M-F in the daily morning bug call and can ensure the bug is redirected to the right team if it isn't handled beforehand. \r\n\r\nDevelopment teams will investigate and identify next steps. Any investigation requiring access will go into a new CR created by the dev team. \r\nAny action recommended by the operations, support or development teams to address the issues will go into the code change pipeline and through the process (Integration, Staging, Prod) to be execute.\r\n\r\n",
        "close_code" : "successful",
        "close_notes" : "CR completed",
        "approval" : "approved",
        "contact_type" : "Manual",
        "priority" : "moderate",
        "service_offering" : "cabiehle@us.ibm.com",
        "sys_class_name" : null,
        "work_notes" : [ ],
        "work_end" : "2020-05-28 20:15:48",
        "calendar_duration" : "",
        "delivery_task" : "CTASK0931304, CTASK0931303",
        "category" : null,
        "opened_at" : "2020-05-28 18:00:00",
        "closed_at" : "2020-05-28 19:00:00",
        "work_start" : "2020-05-28 15:54:23"
    }
}
